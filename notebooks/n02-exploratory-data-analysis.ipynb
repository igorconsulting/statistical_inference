{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Imports and Reading Data\n",
    "\n",
    "This explanation provides a comprehensive overview of the initial setup step, detailing the purpose and actions taken to prepare for the data analysis.\n",
    "\n",
    "In this initial step, we perform the essential setup required for our data analysis project. This includes importing the necessary libraries, configuring settings, and loading our dataset.\n",
    "\n",
    "1. **Importing Libraries**:\n",
    "   - We start by importing the essential libraries that we will use throughout our analysis:\n",
    "     - `pandas` for data manipulation and analysis.\n",
    "     - `os` for interacting with the operating system.\n",
    "     - `numpy` for numerical computations.\n",
    "     - `matplotlib.pylab` for creating visualizations.\n",
    "     - `seaborn` for enhanced data visualizations built on top of matplotlib.\n",
    "   \n",
    "2. **Configuring Settings**:\n",
    "   - We configure `matplotlib` to use the 'ggplot' style for our plots, which provides a clean and visually appealing layout.\n",
    "   - We set the maximum number of columns displayed by `pandas` to 200, ensuring that we can view a large number of columns in our DataFrames without truncation.\n",
    "\n",
    "3. **Library Versions**:\n",
    "   - We define and utilize two functions:\n",
    "     - `get_library_versions(libraries)`: This function takes a list of library names and returns a dictionary containing their respective versions.\n",
    "     - `print_library_versions(versions)`: This function prints the versions of the libraries in a structured format.\n",
    "   - We then create a list of the libraries we have imported and use these functions to display their versions, confirming that the libraries have been loaded successfully.\n",
    "\n",
    "4. **Reading the Data**:\n",
    "   - We specify the relative path to our dataset, `rollercoaster_db.csv`, located in the `data/rollercoaster_db` directory.\n",
    "   - Using the `os.path.basename` function, we extract the file name from the path.\n",
    "   - We read the CSV file into a `pandas` DataFrame using the `pd.read_csv` function.\n",
    "   - Finally, we print a success message indicating that the dataset has been read successfully.\n",
    "\n",
    "By completing these steps, we ensure that our working environment is properly set up, and our data is loaded and ready for analysis. This foundational setup is crucial for maintaining a streamlined and efficient workflow throughout the project.\n",
    "\n",
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries read successfully!\n",
      "- pandas version: 2.2.2\n",
      "- numpy version: 2.0.0\n",
      "- matplotlib version: 3.9.0\n",
      "- seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import importlib.metadata\n",
    "# Configure matplotlib style and pandas options\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "# Get versions of the libraries\n",
    "pandas_version = importlib.metadata.version('pandas')\n",
    "numpy_version = importlib.metadata.version('numpy')\n",
    "matplotlib_version = importlib.metadata.version('matplotlib')\n",
    "seaborn_version = importlib.metadata.version('seaborn')\n",
    "\n",
    "# Print libraries and versions\n",
    "print('Libraries read successfully!')\n",
    "print(f'- pandas version: {pandas_version}')\n",
    "print(f'- numpy version: {numpy_version}')\n",
    "print(f'- matplotlib version: {matplotlib_version}')\n",
    "print(f'- seaborn version: {seaborn_version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries read successfully!\n",
      "- pandas version: 2.2.2\n",
      "- numpy version: 2.0.0\n",
      "- matplotlib version: 3.9.0\n",
      "- seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Modularization\n",
    "\n",
    "def get_library_versions(libraries):\n",
    "    \"\"\"Get the versions of the specified libraries.\"\"\"\n",
    "    versions = {}\n",
    "    for lib in libraries:\n",
    "        try:\n",
    "            versions[lib] = importlib.metadata.version(lib)\n",
    "        except importlib.metadata.PackageNotFoundError:\n",
    "            versions[lib] = 'Not installed'\n",
    "    return versions\n",
    "\n",
    "def print_library_versions(versions):\n",
    "    \"\"\"Print the versions of the libraries.\"\"\"\n",
    "    print('Libraries read successfully!')\n",
    "    for lib, version in versions.items():\n",
    "        print(f'- {lib} version: {version}')\n",
    "\n",
    "# List of libraries to check\n",
    "libraries = ['pandas', 'numpy', 'matplotlib', 'seaborn']\n",
    "\n",
    "# Get and print versions of the libraries\n",
    "versions = get_library_versions(libraries)\n",
    "print_library_versions(versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relative path from the notebook to the CSV file\n",
    "file_path = '../data/rollercoaster_db/coaster_db.csv'\n",
    "# Extract the file name from the path\n",
    "file_name = os.path.basename(file_path)\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "print(f'{file_name} read successsfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Understanding\n",
    "\n",
    "- Dataframe ``shape``\n",
    "- ``head`` and ``tail``\n",
    "- ``dtypes``\n",
    "- ``describe``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we focus on the initial understanding of the dataset. This involves examining the structure, contents, and basic statistics of the data. The following actions are performed:\n",
    "\n",
    "1. **DataFrame Shape**:\n",
    "   - We use the `.shape` attribute of the DataFrame to obtain the dimensions of the dataset. This returns a tuple representing the number of rows and columns, providing an overview of the dataset's size.\n",
    "\n",
    "2. **Head and Tail**:\n",
    "   - We use the `.head()` and `.tail()` methods to display the first few and last few rows of the DataFrame, respectively. This helps us get a sense of the data's structure and contents.\n",
    "\n",
    "3. **Data Types**:\n",
    "   - The `.dtypes` attribute is used to identify the data types of each column in the DataFrame. Understanding the data types is crucial for selecting appropriate data processing and analysis techniques.\n",
    "\n",
    "4. **Descriptive Statistics**:\n",
    "   - The `.describe()` method provides summary statistics for numerical columns in the DataFrame. This includes measures such as mean, median, standard deviation, minimum, and maximum values. These statistics help us understand the distribution and variability of the data.\n",
    "\n",
    "By performing these actions, we gain a preliminary understanding of the dataset's structure, content, and basic statistical properties. This foundational knowledge is essential for informing subsequent steps in the data analysis process, such as data cleaning, transformation, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame Shape\n",
    "print(\"DataFrame Shape:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"Head of the DataFrame:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display last few rows\n",
    "print(\"Tail of the DataFrame:\")\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Types of each column\n",
    "# Every column is actually a series and each series has a type\n",
    "print(\"Data Types:\")\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying categorical (object) and numerical columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "numerical_cols = data.select_dtypes(include=['number']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about categorical columns\n",
    "print(\"Categorical Columns:\")\n",
    "data[categorical_cols].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about numerical columns\n",
    "print(\"Numerical Columns:\")\n",
    "data[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Data Preparation\n",
    "Preparation before analysis\n",
    "\n",
    "- Dropping irrelevant columns and rows.\n",
    "- Identifying duplicated columns.\n",
    "- Renaming columns.\n",
    "- Feature creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data[['coaster_name',\n",
    "      'Location',\n",
    "      'Status',\n",
    "      'Manufacturer',\n",
    "      'year_introduced',\n",
    "      'latitude',\n",
    "      'longitude',\n",
    "      'Type_Main',\n",
    "      'opening_date_clean',\n",
    "      'speed_mph',\n",
    "      'height_ft',\n",
    "      'Inversions_clean',\n",
    "      'Gforce_clean']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of dropping single columns\n",
    "# df.drop(['Opening data'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['opening_date_clean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight: `data_df['opening_date_clean']` is a date! So, we should use the pandas method ``to_datatime`` to modify this data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['opening_date_clean'] = pd.to_datetime(data_df['opening_date_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming using dict data structure\n",
    "\n",
    "data_df = data_df.rename(columns= {'coaster_name': 'Coaster_Name',\n",
    "                        'year_introduced': 'Year_Introduced',\n",
    "                        'opening_date_clean': 'Opening_Date',\n",
    "                        'speed_mph': 'Speed_mph',\n",
    "                        'height_ft': 'Height_ft',\n",
    "                        'Inversions_clean': 'Inversions', \n",
    "                        'Gforce_clean': 'Gforce'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where missing values are?\n",
    "data_df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much now values?\n",
    "data_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is there duplicated data?\n",
    "data_df.loc[data_df.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[data_df.duplicated(subset=['Coaster_Name'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Command\n",
    "\n",
    "Looking why we have duplicated rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking example of duplicate\n",
    "data_df.query('Coaster_Name == \"Crystal Beach Cyclone\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we have duplicated names, it could caused by registered mistakes. Let's look at a column set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.duplicated(subset=['Coaster_Name', 'Location', 'Opening_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.duplicated(subset=['Coaster_Name', 'Location', 'Opening_Date']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with that idea, we can locate entries that are NOT dupliocated using `~` and `.loc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.loc[~data_df.duplicated(subset=['Coaster_Name', 'Location', 'Opening_Date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data_df.loc[~data_df.duplicated(subset=['Coaster_Name', 'Location', 'Opening_Date'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Feature Understanding\n",
    "distributions, outliers,....\n",
    "(Univariate analysis)\n",
    "\n",
    "- plotting Feature Distributions:\n",
    "    - Histogram\n",
    "    - KDE\n",
    "    - Boxplot\n",
    "    - Stripplot (numerical description)\n",
    "    - catplot (``box`` quanto ``bar``)\n",
    "    - boxenplot (a detailed boxplot version)\n",
    "    - violinplot\n",
    "\n",
    "### Univariate Analysis\n",
    "\n",
    "A critical step in any data science project is understanding the features, or variables, in your dataset. Understanding the distribution and characteristics of each feature in your dataset is crucial for building robust and accurate predictive models. Univariate analysis involves examining each variable individually to understand its distribution, central tendency, variability, and presence of outliers. This step helps in identifying potential issues with the data and informs the selection of appropriate modeling techniques. Below, I outline various plotting methods used to understand feature distributions.\n",
    "\n",
    "#### Plotting Feature Distributions\n",
    "\n",
    "1. **Histogram**:\n",
    "   - **Purpose**: To visualize the distribution of numerical data.\n",
    "   - **Description**: Histograms group data into bins and count the number of observations in each bin. This helps in identifying the shape of the data distribution (e.g., normal, skewed, bimodal).\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     import matplotlib.pyplot as plt\n",
    "     import seaborn as sns\n",
    "\n",
    "     sns.histplot(data=your_dataframe, x=\"your_numerical_feature\", kde=False)\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "2. **Kernel Density Estimate (KDE)**:\n",
    "   - **Purpose**: To estimate the probability density function of a continuous variable.\n",
    "   - **Description**: KDE smoothens the observed data points to produce a continuous probability density curve. It provides a more accurate depiction of the data distribution compared to histograms.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.kdeplot(data=your_dataframe[\"your_numerical_feature\"], shade=True)\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "3. **Boxplot**:\n",
    "   - **Purpose**: To summarize the distribution of a dataset.\n",
    "   - **Description**: Boxplots display the median, quartiles, and potential outliers of a dataset. They are particularly useful for identifying the spread and skewness of the data.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.boxplot(data=your_dataframe, x=\"your_numerical_feature\")\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "4. **Stripplot**:\n",
    "   - **Purpose**: To display all individual data points.\n",
    "   - **Description**: Stripplots show each observation along an axis, often overlaid on a boxplot or violinplot to give additional insight into the data distribution and density.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.stripplot(data=your_dataframe, x=\"your_numerical_feature\")\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "5. **Catplot (Box and Bar)**:\n",
    "   - **Purpose**: To analyze and visualize categorical data.\n",
    "   - **Description**:\n",
    "     - **Box Catplot**: Similar to a boxplot but can handle multiple categories and variables.\n",
    "     - **Bar Catplot**: Shows the mean (or other summary statistics) of a numerical variable for each category.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.catplot(data=your_dataframe, x=\"your_categorical_feature\", y=\"your_numerical_feature\", kind=\"box\")\n",
    "     sns.catplot(data=your_dataframe, x=\"your_categorical_feature\", y=\"your_numerical_feature\", kind=\"bar\")\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "6. **Boxenplot**:\n",
    "   - **Purpose**: To provide a more detailed view of the distribution than a standard boxplot.\n",
    "   - **Description**: Boxenplots, or letter-value plots, are similar to boxplots but display more quantiles, giving a more granular view of the data distribution, especially in larger datasets.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.boxenplot(data=your_dataframe, x=\"your_numerical_feature\")\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "7. **Violinplot**:\n",
    "   - **Purpose**: To combine the benefits of boxplots and KDE plots.\n",
    "   - **Description**: Violinplots display the kernel density estimate on each side of a central boxplot. This allows for a deeper understanding of the data distribution and density.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.violinplot(data=your_dataframe, x=\"your_categorical_feature\", y=\"your_numerical_feature\")\n",
    "     plt.show()\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['Year_Introduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = clean_data['Year_Introduced'].value_counts() \\\n",
    "    .head(10) \\\n",
    "        .plot(kind='bar', title = 'Top Years Coasters Introduced')\n",
    "ax.set_xlabel('Year Introduced')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_speed = clean_data['Speed_mph'].plot(kind='hist', bins=50, title = 'Coaster Speed(mph)')\n",
    "ax_speed.set_xlabel('Speed (mph)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commom speed? Outliers?\n",
    "\n",
    "obs: do that in each feature!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_speed_kde = clean_data['Speed_mph'].plot(kind='kde', title = 'Coaster Speed(mph)')\n",
    "ax_speed_kde.set_xlabel('Speed (mph)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Feature Relationships- in general it's characterized by regression and (co)relation analysis\n",
    "\n",
    "- lmplot\n",
    "- residplot\n",
    "- decision tree (evaluate complex relationships e influences of independent variables at dependents)\n",
    "- scatterplot\n",
    "- heatmap correlation\n",
    "- pairplot\n",
    "- groupby comparisons\n",
    "\n",
    "Understanding the relationships between features is essential for building effective predictive models. This step involves analyzing how different variables relate to each other, both in terms of correlation and regression analysis. By exploring these relationships, we can identify potential dependencies, interactions, and influences among variables. Below, I describe various methods for analyzing feature relationships.\n",
    "\n",
    "### Regression and (Co)relation Analysis\n",
    "\n",
    "1. **lmplot**:\n",
    "   - **Purpose**: To visualize linear relationships between two variables.\n",
    "   - **Description**: lmplot combines scatter plots with linear regression lines. It provides insight into the strength and direction of the relationship between two continuous variables.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.lmplot(data=your_dataframe, x=\"independent_variable\", y=\"dependent_variable\")\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "2. **residplot**:\n",
    "   - **Purpose**: To visualize the residuals of a linear regression model.\n",
    "   - **Description**: residplot displays the difference between observed and predicted values (residuals) to assess the fit of a regression model. It helps in diagnosing potential issues like non-linearity, heteroscedasticity, and outliers.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.residplot(data=your_dataframe, x=\"independent_variable\", y=\"dependent_variable\")\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "3. **Decision Tree**:\n",
    "   - **Purpose**: To evaluate complex relationships and the influence of independent variables on dependent variables.\n",
    "   - **Description**: Decision trees split the data based on feature values to predict the target variable. They can capture non-linear relationships and interactions between variables, making them useful for understanding the importance and impact of features.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     from sklearn.tree import DecisionTreeRegressor\n",
    "     from sklearn import tree\n",
    "     import matplotlib.pyplot as plt\n",
    "\n",
    "     model = DecisionTreeRegressor()\n",
    "     model.fit(X, y)\n",
    "     tree.plot_tree(model)\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "4. **Scatterplot**:\n",
    "   - **Purpose**: To visualize the relationship between two continuous variables.\n",
    "   - **Description**: Scatterplots plot individual data points based on their values for two variables. They help in identifying patterns, trends, and potential correlations.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.scatterplot(data=your_dataframe, x=\"variable1\", y=\"variable2\")\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "5. **Heatmap Correlation**:\n",
    "   - **Purpose**: To visualize the correlation matrix of multiple variables.\n",
    "   - **Description**: Heatmaps use color gradients to represent correlation coefficients between pairs of variables. They provide a quick overview of how variables are related and help in identifying strong correlations.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     corr_matrix = your_dataframe.corr()\n",
    "     sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "6. **Pairplot**:\n",
    "   - **Purpose**: To visualize pairwise relationships in a dataset.\n",
    "   - **Description**: Pairplots create scatterplots for every pair of variables and histograms for individual variables. They are useful for exploring potential correlations and interactions in a multidimensional dataset.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     sns.pairplot(your_dataframe)\n",
    "     plt.show()\n",
    "     ```\n",
    "\n",
    "7. **Groupby Comparisons**:\n",
    "   - **Purpose**: To compare summary statistics of different groups within a dataset.\n",
    "   - **Description**: Groupby comparisons involve aggregating data based on categorical variables and computing summary statistics (e.g., mean, median) for numerical variables. This helps in understanding how different groups behave and differ from each other.\n",
    "   - **Example**:\n",
    "     ```python\n",
    "     group_means = your_dataframe.groupby(\"categorical_variable\").mean()\n",
    "     group_means.plot(kind=\"bar\")\n",
    "     plt.show()\n",
    "     ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.plot(kind = 'scatter',\n",
    "                x = 'Speed_mph',\n",
    "                y = 'Height_ft',\n",
    "                title = 'Coaster Speed vs. Height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax_scatter = sns.scatterplot(data = clean_data,\n",
    "                x = 'Speed_mph',\n",
    "                y = 'Height_ft',\n",
    "                hue = 'Year_Introduced')\n",
    "ax_scatter.set_title('Coaster Speed vs. Height')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: Using the variable name `ax` (short for \"axis\") to save plots is a common pattern in data visualization for several reasons. This convention allows for more control over the plot's properties and is especially useful when creating multiple plots or customizing individual plot elements. Hereâ€™s a detailed explanation:\n",
    "\n",
    "### Reasons for Using 'ax' to Save Plots\n",
    "\n",
    "1. **Enhanced Customization**:\n",
    "   Saving the plot to an `ax` variable (an instance of the `Axes` class in Matplotlib) allows you to easily customize various aspects of the plot. You can set titles, labels, legends, and other properties directly on the `ax` object.\n",
    "\n",
    "   ```python\n",
    "   ax_scatter = sns.scatterplot(data=clean_data,\n",
    "                                x='Speed_mph',\n",
    "                                y='Height_ft',\n",
    "                                hue='Year_Introduced')\n",
    "   ax_scatter.set_title('Coaster Speed vs. Height')\n",
    "   ax_scatter.set_xlabel('Speed (mph)')\n",
    "   ax_scatter.set_ylabel('Height (ft)')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "2. **Subplot Management**:\n",
    "   When creating multiple plots in a single figure, using `ax` allows you to manage each subplot individually. This is particularly useful in creating complex visualizations where each subplot needs different customizations.\n",
    "\n",
    "   ```python\n",
    "   fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "   sns.scatterplot(data=clean_data, x='Speed_mph', y='Height_ft', hue='Year_Introduced', ax=ax1)\n",
    "   ax1.set_title('Coaster Speed vs. Height')\n",
    "   \n",
    "   sns.boxplot(data=clean_data, x='Year_Introduced', y='Speed_mph', ax=ax2)\n",
    "   ax2.set_title('Coaster Speed by Year Introduced')\n",
    "   \n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "3. **Consistent Interface**:\n",
    "   Using `ax` provides a consistent interface for interacting with the plot, as it leverages the object-oriented API of Matplotlib. This consistency helps in writing cleaner and more readable code.\n",
    "\n",
    "   ```python\n",
    "   fig, ax = plt.subplots()\n",
    "   ax_scatter = sns.scatterplot(data=clean_data, x='Speed_mph', y='Height_ft', hue='Year_Introduced', ax=ax)\n",
    "   ax.set_title('Coaster Speed vs. Height')\n",
    "   plt.show()\n",
    "   ```\n",
    "\n",
    "4. **Complex Plotting Operations**:\n",
    "   For complex plotting operations, such as adding multiple layers or annotations, having direct access to the `ax` object is essential. It allows for precise placement and customization of these elements.\n",
    "\n",
    "   ```python\n",
    "   ax_scatter = sns.scatterplot(data=clean_data, x='Speed_mph', y='Height_ft', hue='Year_Introduced')\n",
    "   ax_scatter.set_title('Coaster Speed vs. Height')\n",
    "   \n",
    "   # Adding a horizontal line\n",
    "   ax_scatter.axhline(y=200, color='red', linestyle='--')\n",
    "   \n",
    "   # Adding a vertical line\n",
    "   ax_scatter.axvline(x=50, color='blue', linestyle='--')\n",
    "   \n",
    "   plt.show()\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more than two features\n",
    "sns.pairplot(data = clean_data,\n",
    "             vars = ['Year_Introduced','Speed_mph','Height_ft','Inversions','Gforce'],\n",
    "             hue='Type_Main')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['Year_Introduced','Speed_mph','Height_ft','Inversions','Gforce']\n",
    "\n",
    "clean_data[vars].dropna().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_corr = clean_data[vars].dropna().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(clean_data_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(clean_data_corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Ask a Question about the data\n",
    "\n",
    "- try to answer a question you have about the data using a plot or statistic.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
